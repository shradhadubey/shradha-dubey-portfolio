{
    "name": "SHRADHA DUBEY",
    "role": "AWS Data Engineer", 
    "resume_file": "docs/Shradha_Dubey_Resume.pdf",
    "linkedin": "https://www.linkedin.com/in/shradhadubey/",
    "location": "Indore, India(UTC +5:30) (Open to Relocate Worldwide)",
    "email": "shradha.dubeyy@gmail.com",
    "summary": "Big Data Engineer with over 6 years of experience designing and optimizing scalable data solutions, including AWS cloud architecture and big data engineering. Expertise in AWS, Pyspark, SQL, and Python.",
    "certifications": [
    {
        "name": "AWS Solution Architect Associate (SAA-02)",
        "image": "certificates/aws-saa.png"
    },
    {
        "name": "AWS Cloud Practitioner (CLF-C01)",
        "image": "certificates/aws-cloud-practitioner.png"
    },
    {
        "name": "Certification of Advance Studies in Data Science",
        "image": "certificates/data-science.png"
    }
    ],
    "skills": {
        "languages": ["Python", "SQL", "Scala"],
        "tools": ["Apache Spark", "Airflow", "Docker", "AWS", "dbt"],
        "databases": ["PostgreSQL", "MongoDB", "Redshift"],
        "cloud": ["Cloud ETL Pipeline", "AWS Lambda", "S3", "Athena", "Glue Catalog", "Glue ETL", "Step Functions", "CloudWatch", "IAM", "SNS"]
},
    "experience": [
        {
            "role": "AWS Data Engineer",
            "profession": "Freelancer, Hyderabad, India",
            "duration": "Jan 25 - Current",
            "achievements": [
                "Designed and implemented scalable AWS cloud architectures, ensuring high availability, security, and cost efficiency.",
                "Developed automated ELT pipelines using AWS Glue, Lambda, and Redshift, optimizing data ingestion & transformation.",
                "Architected serverless data processing solutions leveraging AWS Step Functions and S3, reducing infrastructure overhead.",
                "Led cloud migration strategies, integrating AWS services like EC2 & DynamoDB for enhanced scalability and performance.",
                "Established IAM policies and security best practices, ensuring compliance with AWS security standards."
            ]
        },
        {
            "role": "Big Data Engineer",
            "company": "Rocket Mortgage, Detroit, Michigan",
            "duration": "Jun 21 - Sept 24",
            "achievements": [
                "Built AWS-based data pipelines, reducing data processing time by 15%, enabling real-time insights.",
                "Optimized data lake storage and query performance through EMR & Glue, reducing storage costs by 20%.",
                "Built automated data workflows & ETL pipelines, integrating data into a centralized data lake.",
                "Developed data models and data marts, improving query efficiency by 25%.",
                "Collaborated with cross-functional teams to implement data governance policies."
            ]
        },
        {
            "role": "Associate Big Data Engineer",
            "company": "Rocket Mortgage, Detroit, Michigan",
            "duration": "Dec 19 - Jun 21",
            "achievements": [
                "Led migration of legacy data systems to AWS, improving system scalability and reducing downtime by 10%.",
                "Integrated deployments for CICD pipelines that helped in faster deployment of IAC and facilitated rapid testing.",
                "Developed & optimized ETL processes for large-scale data transformations from TBs of datasets.",
                "Worked with Spark & Hadoop ecosystems for distributed data processing, achieving 20% improvement.",
                "Streamlined data ingestion pipelines by introducing automation that reduced manual intervention by 30%.",
                "Executed data-profiling systems to identify inconsistencies, improving data accuracy and reliability."
            ]
        },
        {
            "role": "Data Analyst Intern",
            "company": "Quicken Loans, Detroit, Michigan",
            "duration": "Jun 19 - Dec 19",
            "achievements": [
                "Performed detailed data analysis on large datasets, driving a 10% decrease in operation cost.",
                "Created interactive dashboards using Tableau, reducing report generation time by 25%.",
                "Developed custom SQL queries to analyze & retrieve data, optimizing query performance by 15%.",
                "Partnered with business units to identify KPIs and built visualizations to track progress."
            ]
        }
    ],
    "education": [
        {
            "degree": "MS in Information Management & Data Science",
            "institution": "Syracuse University, Syracuse, New York, USA",
            "gpa": "3.56",
            "coursework": "Data Analytics, Data Mining, Data Modeling, Database Management, Information Visualization"
        },
        {
            "degree": "PG Diploma in Computer Application",
            "institution": "DAVV, Indore, India",
            "gpa": "3.50",
            "coursework": "System Analysis & Design, Web Designing, Internet & E-Commerce, HTML, CSS, C#, C++"
        }
    ],
    "projects": [
        {
            "title": "Cloud ETL Pipeline",
            "description": "Real-time data ingestion system using Python and AWS.",
            "impact_metrics": [
                {"label": "Records / Day", "value": 1200000, "suffix": "+", "display": "1.2M+"},
                {"label": "Latency (Seconds)", "value": 2, "suffix": "s", "display": "< 2s"},
                {"label": "System Uptime", "value": 99.9, "suffix": "%", "display": "99.9%"},
                {"label": "Storage Cost Reduction", "value": 20, "suffix": "%", "display": "20%"}
            ],
            "tech": ["Python", "AWS Lambda", "S3", "Athena", "Glue Catalog", "Glue ETL", "Step Functions", "CloudWatch", "IAM", "SNS"]
        }
    ]
}
